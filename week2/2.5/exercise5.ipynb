{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37564bitbiocontrolconda7b5fee27020548c79a893d79a1547dfe",
   "display_name": "Python 3.7.5 64-bit ('biocontrol': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2.5: Introduction to CMAC\n",
    "The CMAC controller consists of a series of \"receptive fields\" (RFs) which can be activated by the input. The idea of the CMAC is to act like a kind of adaptive memory which can be used for feedforward control. Each receptive field is only activated in a small area of the input space.\n",
    "\n",
    "We will begin with the case of a single input and output dimension. Your first task is to use the CMAC to learn a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "For a single input dimension, the activation of the receptive fields is computed with Gaussian basis functions like so:\n",
    "\\begin{equation}\n",
    "    \\phi_i(x) = \\exp\\left(-\\frac{(x-\\mu_i)^2}{\\sigma^2} \\right), \\; i = 1, 2, \\dots, N.\n",
    "\\end{equation}\n",
    "The activation of a receptive field, $\\phi_i(x)$, is a number between 0 and 1. A value close to 0 means that the particular receptive field does not match the given input, and a value close to 1 means that the particular receptive field contains information (a memory) about the given input $x$.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "To get started, we define a number of receptive fields $N=5$, and distribute the mean values $\\mu_i$ evenly in the interval $[0,2\\pi]$. We also select a value for the width $\\sigma$.\n",
    "\n",
    "Below you see a plot of all of the receptive fields in one figure with x on the x-axis and $\\phi_i$ on the y-axis. How much do they overlap? Try to change $\\sigma$ and see what happens. How does the overlap depend on your choice of $\\sigma$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin = 0\n",
    "xmax = 2*np.pi\n",
    "\n",
    "N = 5\n",
    "mu = np.linspace(xmin, xmax, N)\n",
    "\n",
    "sigma = 0.5\n",
    "\n",
    "n_plot = 10000\n",
    "xplot = np.linspace(0, 2*np.pi, n_plot)\n",
    "\n",
    "plt.figure()\n",
    "for i in range(N):\n",
    "    phi = np.exp(-(xplot-mu[i])**2/sigma**2)\n",
    "    plt.plot(xplot, phi, label='RF ' + str(i))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('$\\phi$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We will now initialize a vector $\\mathbf{w}$ of size $N$. These are the weights that will be learned, carrying the memory. The output is now given as\n",
    "    \\begin{equation}\n",
    "        \\hat{y} = \\sum_{i=1}^N w_i \\phi_i = \\mathbf{w}^T \\phi,\n",
    "    \\end{equation}\n",
    "where the last $\\phi$ is a vector containing all the receptive field activations $\\phi_i$.\n",
    "\n",
    "We initialize the weights randomly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.normal(loc=0.0, scale=0.1,size=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We can plot the output $\\hat{y}$ for the given input range of x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.zeros(n_plot)\n",
    "for i in range(N):\n",
    "    y += w[i]*np.exp(-(xplot-mu[i])**2/sigma**2)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xplot, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "At this point the output is completely random.\n",
    "\n",
    "We will attempt to learn the function $\\sin(x)$. To update the receptive fields, we will use the covariance learning rule, which updates the weights using a single training example $(x, y_d)$ where $y_d$ is the desired output. Given the error $e = y_d - \\hat{y}$, the update rule is:\n",
    "    \n",
    "\\begin{equation}\n",
    "    w_i^{(k+1)} = w_i^{(k)} +\\beta e \\phi_i,\n",
    "\\end{equation}\n",
    "where $w_i^{(k)}$ is the $i$'th weight in the $k$'th iteration and $\\beta$ is the learning rate.\n",
    "\n",
    "We generate a number of examples, $(x, y_d)$, where $y_d = \\sin(x)$. The range of $x$ values is in the same range as used for the receptive fields. For each example we:\n",
    " - calculate the CMAC output, $\\hat{y}$.\n",
    " - calculate the associated error, $e$, and save it in a vector.\n",
    " - update the weights using the covariance learning rule (using $\\beta=10^{-3}$).\n",
    "***\n",
    "Lastly, we calculate the mean square error (MSE) of all the examples. We run through all of the examples a number of times, and plot the estimated function together with $\\sin(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.normal(loc=0.0, scale=0.1,size=N)\n",
    "beta = 1e-3\n",
    "\n",
    "n_epochs = 100\n",
    "n_examples = 100\n",
    "\n",
    "x = np.linspace(0, 2*np.pi, n_examples)\n",
    "y = np.sin(x)\n",
    "\n",
    "e_vec = []\n",
    "\n",
    "for _ in range(n_epochs): # in each epoch\n",
    "    es = []\n",
    "    for k in range(x.shape[0]): # for every example\n",
    "        phi = np.exp(-(x[k]-mu)**2/sigma**2) # for all receptive fields at once\n",
    "\n",
    "        yhat = np.dot(w, phi)\n",
    "        e = y[k] - yhat \n",
    "        es.append(e**2)\n",
    "\n",
    "        w += beta*e*phi # for all weights\n",
    "\n",
    "    e_vec.append(np.mean(np.array(es)))\n",
    "\n",
    "y = np.zeros(n_plot)\n",
    "for i in range(N):\n",
    "    y += w[i]*np.exp(-(xplot-mu[i])**2/sigma**2)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xplot, y, label='CMAC')\n",
    "plt.plot(xplot, np.sin(xplot), label='Target')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(e_vec)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Run the above code a number of times with different parameters. Answer the following questions:\n",
    " - What is a good value for $\\sigma$?\n",
    " - After how many \"epochs\" does the learning converge?\n",
    " - Does it make a difference in what order the examples are given, or how many there are?\n",
    "\n",
    "Increase the number of receptive fields. How many is needed for the MSE to reach below $10^{-5}$ ?"
   ]
  }
 ]
}